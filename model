Andrysdf WertoLab, [4 дек. 2021 г., 15:58:38]:
...mport tensorflow as tf 
from keras.layers import BatchNormalization, LeakyReLU, Add, Dropout, Conv2DTranspose, concatenate, MaxPooling2D,UpSampling2D 
from tensorflow.keras.layers import Input, Conv2D 
from tensorflow.keras.layers import MaxPool2D, Flatten, Dense , GlobalMaxPool2D,GlobalAveragePooling2D 
from tensorflow.keras import Model 
import cv2 
from keras import backend as K 
import os 
import numpy as np 
import sklearn.model_selection as sk 
from tensorflow.keras.optimizers import SGD,Nadam,Adam 
#from imutils import paths 
import numpy as np 
from numpy import load 
 
data = load('C:/My_folder/to_task_1/train_data/train_data/binary.npz') 
''' 
for item in lst: 
    print(item) 
    #print(data[item]) 
 
''' 
array = data['1_0.JPG'].astype(int) 
print(array) 
lst = data.files 
#new = np.zeros((3024,4032)) 
 
x = [] 
y = [] 
#print(array.shape) 
 
 
for item in lst: 
  ''' 
  #new = np.zeros((3024,4032)) 
  a = data[item] 
  b = data[item].shape 
  new = np.zeros((b[0],b[1])) 
  print(b) 
  for i in range(b[0]): 
    for j in range(b[1]): 
      if(a[i][j] == True): 
        new[i][j] = 1 
      else: 
        new[i][j] = 0 
  ''' 
  y.append(cv2.resize(data[item].astype(float), (512, 512))) 
  a = cv2.imread('C:/My_folder/to_task_1/train_data/train_data/images/' + item) 
  print(a) 
  x.append(cv2.resize(a, (512, 512))) 
  print(1)#np.count_nonzero(data[item].astype(int)==1)) 
print(len(x)) 
print(len(y)) 
X_train, X_test, y_train, y_test = sk.train_test_split(x,y,test_size=0.2) 
X_train = np.asarray(X_train) 
y_train = np.asarray(y_train) 
cv2.imshow("wewe", x[0]) 
 
 
 
def dice_coef(y_true, y_pred): 
  y_true_f = K.flatten(y_true) 
  y_pred = K.cast(y_pred, 'float32') 
  y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32') 
  intersection = y_true_f * y_pred_f 
  score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f)) 
  return score 
 
 
def dice_loss(y_true, y_pred): 
  smooth = 1. 
  y_true_f = K.flatten(y_true) 
  y_pred_f = K.flatten(y_pred) 
  intersection = y_true_f * y_pred_f 
  score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) 
  return 1. - score 
 
 
def bce_dice_loss(y_true, y_pred): 
  return  dice_loss(y_true, y_pred)#+binary_crossentropy(y_true, y_pred) 
 
 
def convolution_block(x, filters, size, strides=(1, 1), padding='same', activation=True): 
  x = Conv2D(filters, size, strides=strides, padding=padding)(x) 
  x = BatchNormalization()(x) 
  if activation == True: 
    x = LeakyReLU(alpha=0.1)(x) 
  return x 
 
 
def residual_block(blockInput, num_filters=16): 
  x = LeakyReLU(alpha=0.1)(blockInput) 
  x = BatchNormalization()(x) 
  blockInput = BatchNormalization()(blockInput) 
  x = convolution_block(x, num_filters, (3, 3)) 
  x = convolution_block(x, num_filters, (3, 3), activation=False) 
  x = Add()([x, blockInput]) 
  return x 
 
 
from tensorflow.keras.applications.efficientnet import EfficientNetB4 
 
 
def UEfficientNet(input_shape=(None, None, 3), dropout_rate=0.1): 
  backbone = EfficientNetB4(weights='imagenet', 
                            include_top=False, 
                            input_shape=input_shape) 
 
  input = backbone.output 
 
  start_neurons = 8 
 
  conv4 = backbone.layers[342].output 
  conv4 = LeakyReLU(alpha=0.1)(conv4) 
  pool4 = MaxPooling2D((2, 2))(conv4) 
  pool4 = Dropout(dropout_rate)(pool4) 
 
  # Middle 
  convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding="same", name='conv_middle')(pool4) 
  convm = residual_block(convm, start_neurons * 32) 
  convm = residual_block(convm, start_neurons * 32) 
  convm = LeakyReLU(alpha=0.1)(convm) 
 
  deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding="same")(convm) 
  deconv4_up1 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding="same")(deconv4) 
  deconv4_up2 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding="same")(deconv4_up1) 
  deconv4_up3 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding="same")(deconv4_up2) 
  uconv4 =

concatenate([deconv4, conv4]) 
  uconv4 = Dropout(dropout_rate)(uconv4) 
 
  uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding="same")(uconv4) 
  uconv4 = residual_block(uconv4, start_neurons * 16) 
  #     uconv4 = residual_block(uconv4,start_neurons * 16) 
  uconv4 = LeakyReLU(alpha=0.1)(uconv4)  # conv1_2 
 
  deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(uconv4) 
  deconv3_up1 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(deconv3) 
  deconv3_up2 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(deconv3_up1) 
  conv3 = backbone.layers[15].output 
  uconv3 = concatenate([deconv3, deconv4_up1])#, conv3]) 
  uconv3 = Dropout(dropout_rate)(uconv3) 
 
  uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding="same")(uconv3) 
  uconv3 = residual_block(uconv3, start_neurons * 8) 
  #     uconv3 = residual_block(uconv3,start_neurons * 8) 
  uconv3 = LeakyReLU(alpha=0.1)(uconv3) 
 
  deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(uconv3) 
  deconv2_up1 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(deconv2) 
  conv2 = backbone.layers[92].output 
  uconv2 = concatenate([deconv2, deconv3_up1, deconv4_up2, conv2]) 
 
  uconv2 = Dropout(0.1)(uconv2) 
  uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding="same")(uconv2) 
  uconv2 = residual_block(uconv2, start_neurons * 4) 
  #     uconv2 = residual_block(uconv2,start_neurons * 4) 
  uconv2 = LeakyReLU(alpha=0.1)(uconv2) 
 
  deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding="same")(uconv2) 
  conv1 = backbone.layers[30].output 
  uconv1 = concatenate([deconv1, deconv2_up1, deconv3_up2, deconv4_up3])#, conv1]) 
 
  uconv1 = Dropout(0.1)(uconv1) 
  uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding="same")(uconv1) 
  uconv1 = residual_block(uconv1, start_neurons * 2) 
  #     uconv1 = residual_block(uconv1,start_neurons * 2) 
  uconv1 = LeakyReLU(alpha=0.1)(uconv1) 
 
  uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding="same")(uconv1) 
  uconv0 = Dropout(0.1)(uconv0) 
  uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding="same")(uconv0) 
  uconv0 = residual_block(uconv0, start_neurons * 1) 
  #     uconv0 = residual_block(uconv0,start_neurons * 1) 
  uconv0 = LeakyReLU(alpha=0.1)(uconv0) 
 
  uconv0 = Dropout(dropout_rate / 2)(uconv0) 
  output_layer = Conv2DTranspose(1, (1, 1),strides=(2, 2), padding="same", activation="sigmoid")(uconv0) 
  model = Model(inputs = backbone.input, outputs = output_layer) 
  return model 
 
adam = tf.keras.optimizers.Adam(lr=0.0004, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) 
from keras import backend as K 
def recall_m(y_true, y_pred): 
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) 
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) 
    recall = true_positives / (possible_positives + K.epsilon()) 
    return recall 
 
def precision_m(y_true, y_pred): 
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) 
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) 
    precision = true_positives / (predicted_positives + K.epsilon()) 
    return precision 
 
def f1_m(y_true, y_pred): 
    precision = precision_m(y_true, y_pred) 
    recall = recall_m(y_true, y_pred) 
    return 2*((precision*recall)/(precision+recall+K.epsilon())) 
 
# compile the model 
K.clear_session() 
img_size = 512 
model = UEfficientNet(input_shape=(img_size, img_size, 3), dropout_rate=0.5) 
print(model.summary()) 
model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[f1_m,'accuracy']) 
#model.compile(loss=bce_dice_loss, optimizer='adam') 
model.fit(X_train, y_train, 
          batch_size=2, 
          epochs = 3, 
          validation_data=(X_test, y_test)) 
 
cv2.waitKey(0)
